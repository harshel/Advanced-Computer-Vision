{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORT THE LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import torch\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEFINING THE VGG16 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HEqsV8KMK29H"
   },
   "outputs": [],
   "source": [
    "\n",
    "# define the VGG\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        \n",
    "        # load the vgg model's features\n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "    \n",
    "    def get_content_activations(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "            Extracts the features for the content loss from the block4_conv2 of VGG19\n",
    "            Args:\n",
    "                x: torch.Tensor - input image we want to extract the features of\n",
    "            Returns:\n",
    "                features: torch.Tensor - the activation maps of the block4_conv2 layer\n",
    "        \"\"\"\n",
    "        features = self.vgg[:23](x)\n",
    "        return features\n",
    "    \n",
    "    def get_style_activations(self, x):\n",
    "        \"\"\"\n",
    "            Extracts the features for the style loss from the block1_conv1, \n",
    "                block2_conv1, block3_conv1, block4_conv1, block5_conv1 of VGG19\n",
    "            Args:\n",
    "                x: torch.Tensor - input image we want to extract the features of\n",
    "            Returns:\n",
    "                features: list - the list of activation maps of the block1_conv1, \n",
    "                    block2_conv1, block3_conv1, block4_conv1, block5_conv1 layers\n",
    "        \"\"\"\n",
    "        features = [self.vgg[:4](x)] + [self.vgg[:7](x)] + [self.vgg[:12](x)] + [self.vgg[:21](x)] + [self.vgg[:30](x)] \n",
    "        return features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.vgg(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEFINING THE LOSS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GH6fakD0LBOj"
   },
   "outputs": [],
   "source": [
    "def gram(tensor):\n",
    "    \"\"\"\n",
    "        Constructs the Gramian matrix out of the tensor\n",
    "    \"\"\"\n",
    "    return torch.mm(tensor, tensor.t())\n",
    "\n",
    "\n",
    "def gram_loss(noise_img_gram, style_img_gram, N, M):\n",
    "    \"\"\"\n",
    "        Gramian loss: the SSE between Gramian matrices of a layer\n",
    "            arXiv:1508.06576v2 - equation (4)\n",
    "    \"\"\"\n",
    "    return torch.sum(torch.pow(noise_img_gram - style_img_gram, 2)).div((np.power(N*M*2, 2, dtype=np.float64)))\n",
    "\n",
    "\n",
    "def total_variation_loss(image):\n",
    "    \"\"\"\n",
    "        Variation loss makes the images smoother, defined over spacial dimensions\n",
    "    \"\"\"\n",
    "    loss = torch.mean(torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:])) + \\\n",
    "        torch.mean(torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :]))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def content_loss(noise: torch.Tensor, image: torch.Tensor):\n",
    "    \"\"\"\n",
    "        Simple SSE loss over the generated image and the content image\n",
    "            arXiv:1508.06576v2 - equation (1)\n",
    "    \"\"\"\n",
    "    return 1/2. * torch.sum(torch.pow(noise - image, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xevv1Tp2Ln1U"
   },
   "outputs": [],
   "source": [
    "def main(style_img_path: str,\n",
    "         content_img_path: str, \n",
    "         img_dim: int,\n",
    "         num_iter: int,\n",
    "         style_weight: int,\n",
    "         content_weight: int,\n",
    "         variation_weight: int,\n",
    "         print_every: int,\n",
    "         save_every: int):\n",
    "\n",
    "    assert style_img_path is not None\n",
    "    assert content_img_path is not None\n",
    "\n",
    "    # define the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # read the images\n",
    "    style_img = Image.open(style_img_path)\n",
    "    cont_img = Image.open(content_img_path)\n",
    "    \n",
    "    # define the transform\n",
    "    transform = transforms.Compose([transforms.Resize((img_dim, img_dim)),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                         [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    # get the tensor of the image\n",
    "    content_image = transform(cont_img).unsqueeze(0).to(device)\n",
    "    style_image = transform(style_img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # init the network\n",
    "    vgg = VGG().to(device).eval()\n",
    "    \n",
    "    # replace the MaxPool with the AvgPool layers\n",
    "    for name, child in vgg.vgg.named_children():\n",
    "        if isinstance(child, nn.MaxPool2d):\n",
    "            vgg.vgg[int(name)] = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            \n",
    "    # lock the gradients\n",
    "    for param in vgg.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # get the content activations of the content image and detach them from the graph\n",
    "    content_activations = vgg.get_content_activations(content_image).detach()\n",
    "    \n",
    "    # unroll the content activations\n",
    "    content_activations = content_activations.view(512, -1)\n",
    "    \n",
    "    # get the style activations of the style image\n",
    "    style_activations = vgg.get_style_activations(style_image)\n",
    "    \n",
    "    # for every layer in the style activations\n",
    "    for i in range(len(style_activations)):\n",
    "\n",
    "        # unroll the activations and detach them from the graph\n",
    "        style_activations[i] = style_activations[i].squeeze().view(style_activations[i].shape[1], -1).detach()\n",
    "\n",
    "    # calculate the gram matrices of the style image\n",
    "    style_grams = [gram(style_activations[i]) for i in range(len(style_activations))]\n",
    "    \n",
    "    # generate the Gaussian noise\n",
    "    noise = torch.randn(1, 3, img_dim, img_dim, device=device, requires_grad=True)\n",
    "    \n",
    "    # define the adam optimizer\n",
    "    # pass the feature map pixels to the optimizer as parameters\n",
    "    adam = optim.Adam(params=[noise], lr=0.01, betas=(0.9, 0.999))\n",
    "\n",
    "    # run the iteration\n",
    "    for iteration in range(num_iter):\n",
    "\n",
    "        # zero the gradient\n",
    "        adam.zero_grad()\n",
    "\n",
    "        # get the content activations of the Gaussian noise\n",
    "        noise_content_activations = vgg.get_content_activations(noise)\n",
    "\n",
    "        # unroll the feature maps of the noise\n",
    "        noise_content_activations = noise_content_activations.view(512, -1)\n",
    "\n",
    "        # calculate the content loss\n",
    "        content_loss_ = content_loss(noise_content_activations, content_activations)\n",
    "\n",
    "        # get the style activations of the noise image\n",
    "        noise_style_activations = vgg.get_style_activations(noise)\n",
    "\n",
    "        # for every layer\n",
    "        for i in range(len(noise_style_activations)):\n",
    "\n",
    "            # unroll the the noise style activations\n",
    "            noise_style_activations[i] = noise_style_activations[i].squeeze().view(noise_style_activations[i].shape[1], -1)\n",
    "\n",
    "        # calculate the noise gram matrices\n",
    "        noise_grams = [gram(noise_style_activations[i]) for i in range(len(noise_style_activations))]\n",
    "\n",
    "        # calculate the total weighted style loss\n",
    "        style_loss = 0\n",
    "        for i in range(len(style_activations)):\n",
    "            N, M = noise_style_activations[i].shape[0], noise_style_activations[i].shape[1]\n",
    "            style_loss += (gram_loss(noise_grams[i], style_grams[i], N, M) / 5.)\n",
    "\n",
    "        # put the style loss on device\n",
    "        style_loss = style_loss.to(device)\n",
    "            \n",
    "        # calculate the total variation loss\n",
    "        variation_loss = total_variation_loss(noise).to(device)\n",
    "\n",
    "        # weight the final losses and add them together\n",
    "        total_loss = content_weight * content_loss_ + style_weight * style_loss + variation_weight * variation_loss\n",
    "\n",
    "        if iteration % print_every == 0:\n",
    "            print(\"Iteration: {}, Content Loss: {:.3f}, Style Loss: {:.3f}, Var Loss: {:.3f}\".format(iteration, \n",
    "                                                                                                     content_weight * content_loss_.item(),\n",
    "                                                                                                     style_weight * style_loss.item(), \n",
    "                                                                                                     variation_weight * variation_loss.item()))\n",
    "\n",
    "        # create the folder for the generated images\n",
    "        if not os.path.exists('./generated/'):\n",
    "            os.mkdir('./generated/')\n",
    "        \n",
    "        # generate the image\n",
    "        if iteration % save_every == 0:\n",
    "            save_image(noise.cpu().detach(), filename='./generated/iter_{}.png'.format(iteration))\n",
    "\n",
    "        # backprop\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        adam.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "XaRtt96IMkPk",
    "outputId": "7b26bd43-35c6-40bb-a2ee-5b229914e899"
   },
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S9uOVjojNlwI",
    "outputId": "61d71226-6fc6-4ed0-d2f9-f10b02695997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StyleTransfer.ipynb\n",
      "content_img_1.jpg\n",
      "style_img_1.jpeg\n",
      "style_img_2.jpg\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "5PflNHxPPAW3",
    "outputId": "614820f7-3cd2-4fc8-9489-74e2d34b6fe7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to C:\\Users\\Harshal/.cache\\torch\\checkpoints\\vgg19-dcbb9e9d.pth\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 548M/548M [02:03<00:00, 4.65MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Content Loss: 416.564, Style Loss: 3927154.839, Var Loss: 22578.158\n",
      "Iteration: 500, Content Loss: 926.612, Style Loss: 142313.084, Var Loss: 18363.991\n",
      "Iteration: 1000, Content Loss: 979.907, Style Loss: 57351.720, Var Loss: 16788.577\n",
      "Iteration: 1500, Content Loss: 966.062, Style Loss: 16623.667, Var Loss: 14469.128\n",
      "Iteration: 2000, Content Loss: 937.888, Style Loss: 8590.085, Var Loss: 11433.748\n",
      "Iteration: 2500, Content Loss: 917.420, Style Loss: 5647.683, Var Loss: 8440.204\n",
      "Iteration: 3000, Content Loss: 897.237, Style Loss: 4076.416, Var Loss: 5977.107\n",
      "Iteration: 3500, Content Loss: 876.495, Style Loss: 3080.994, Var Loss: 4314.184\n",
      "Iteration: 4000, Content Loss: 856.655, Style Loss: 2387.100, Var Loss: 3394.766\n",
      "Iteration: 4500, Content Loss: 836.788, Style Loss: 1880.576, Var Loss: 2936.953\n",
      "Iteration: 5000, Content Loss: 817.842, Style Loss: 1498.656, Var Loss: 2695.118\n",
      "Iteration: 5500, Content Loss: 799.770, Style Loss: 1203.540, Var Loss: 2547.415\n",
      "Iteration: 6000, Content Loss: 782.146, Style Loss: 976.347, Var Loss: 2445.101\n",
      "Iteration: 6500, Content Loss: 765.504, Style Loss: 801.402, Var Loss: 2369.325\n",
      "Iteration: 7000, Content Loss: 749.612, Style Loss: 667.523, Var Loss: 2306.592\n",
      "Iteration: 7500, Content Loss: 735.830, Style Loss: 567.459, Var Loss: 2253.087\n",
      "Iteration: 8000, Content Loss: 722.338, Style Loss: 495.135, Var Loss: 2208.768\n",
      "Iteration: 8500, Content Loss: 710.577, Style Loss: 444.390, Var Loss: 2172.084\n",
      "Iteration: 9000, Content Loss: 700.350, Style Loss: 409.473, Var Loss: 2142.527\n",
      "Iteration: 9500, Content Loss: 691.970, Style Loss: 386.437, Var Loss: 2119.083\n",
      "Iteration: 10000, Content Loss: 686.270, Style Loss: 368.814, Var Loss: 2100.735\n",
      "Iteration: 10500, Content Loss: 681.224, Style Loss: 358.039, Var Loss: 2087.900\n",
      "Iteration: 11000, Content Loss: 676.108, Style Loss: 350.400, Var Loss: 2077.176\n",
      "Iteration: 11500, Content Loss: 674.039, Style Loss: 344.868, Var Loss: 2067.849\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "style_img = 'style_img_1.jpeg'\n",
    "content_img = 'content_img_1.jpg'\n",
    "\n",
    "main(style_img, content_img, 512, 12000, 10e6, 10e-4, 10e3, 500, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "DWazIvW21brs",
    "outputId": "bc3e44b9-ef9f-4e50-ca9a-84afad61459b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_0.png     iter_200.png   iter_400.png   iter_600.png\n",
      "iter_1000.png  iter_3000.png  iter_5000.png  iter_7000.png\n",
      "iter_100.png   iter_300.png   iter_500.png   iter_8000.png\n",
      "iter_2000.png  iter_4000.png  iter_6000.png  iter_9000.png\n"
     ]
    }
   ],
   "source": [
    "!ls /content/generated/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x9TkhXjr1nYr"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('/content/generated/iter_8000.png')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "StyleTransfer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
