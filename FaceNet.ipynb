{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORT THE LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x7HjVWQnYsjv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import model as embedding\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LEECKCVJtmp"
   },
   "outputs": [],
   "source": [
    "#upload model.py, deploy.prototxt.txt, res10, dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EsOcygsQ7SfS",
    "outputId": "91ed3bab-5d66-4e32-b5bf-fb72da712e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  dataset.zip\n",
      "   creating: dataset/APJ Abdul Kalam/\n",
      "  inflating: dataset/APJ Abdul Kalam/kalam-1.jpeg  \n",
      "  inflating: dataset/APJ Abdul Kalam/kalam-10.jpg  \n",
      "  inflating: dataset/APJ Abdul Kalam/kalam-2.jpeg  \n",
      "  inflating: dataset/APJ Abdul Kalam/kalam-3.jpeg  \n",
      "  inflating: dataset/APJ Abdul Kalam/kalam-4.jpeg  \n",
      "  inflating: dataset/APJ Abdul Kalam/kalam-5.jpeg  \n",
      "  inflating: dataset/APJ Abdul Kalam/kalam-6.jpg  \n",
      "  inflating: dataset/APJ Abdul Kalam/kalam-7.jpg  \n",
      "  inflating: dataset/APJ Abdul Kalam/kalam-8.jpg  \n",
      "  inflating: dataset/APJ Abdul Kalam/kalam-9.jpg  \n",
      "   creating: dataset/Ashwin/\n",
      "  inflating: dataset/Ashwin/ashwin-1.jpeg  \n",
      "  inflating: dataset/Ashwin/ashwin-10.jpg  \n",
      "  inflating: dataset/Ashwin/ashwin-3.jpg  \n",
      "  inflating: dataset/Ashwin/ashwin-4.jpg  \n",
      "  inflating: dataset/Ashwin/ashwin-5.jpg  \n",
      "  inflating: dataset/Ashwin/ashwin-6.jpg  \n",
      "  inflating: dataset/Ashwin/ashwin-7.jpg  \n",
      "  inflating: dataset/Ashwin/ashwin-8.jpg  \n",
      "  inflating: dataset/Ashwin/ashwin-9.jpg  \n",
      "   creating: dataset/B kumar/\n",
      "  inflating: dataset/B kumar/bk-10.jpg  \n",
      "  inflating: dataset/B kumar/bk-2.jpg  \n",
      "  inflating: dataset/B kumar/bk-3.jpeg  \n",
      "  inflating: dataset/B kumar/bk-5.jpg  \n",
      "  inflating: dataset/B kumar/bk-6.jpg  \n",
      "  inflating: dataset/B kumar/bk-7.jpg  \n",
      "  inflating: dataset/B kumar/bk-8.jpg  \n",
      "  inflating: dataset/B kumar/bk-9.jpg  \n",
      "   creating: dataset/bumrah/\n",
      "  inflating: dataset/bumrah/bumrah.jpg  \n",
      "  inflating: dataset/bumrah/bumrah-1.jpg  \n",
      "  inflating: dataset/bumrah/bumrah-10.jpg  \n",
      "  inflating: dataset/bumrah/bumrah-4.jpg  \n",
      "  inflating: dataset/bumrah/bumrah-5.jpg  \n",
      "  inflating: dataset/bumrah/bumrah-6.jpg  \n",
      "  inflating: dataset/bumrah/bumrah-7.jpg  \n",
      "  inflating: dataset/bumrah/bumrah-8.jpg  \n",
      "  inflating: dataset/bumrah/bumrah-9.jpg  \n",
      "   creating: dataset/dhawan/\n",
      " extracting: dataset/dhawan/dhawan-1.jpeg  \n",
      "  inflating: dataset/dhawan/dhawan-2.jpeg  \n",
      "  inflating: dataset/dhawan/dhawan-3.jpeg  \n",
      "  inflating: dataset/dhawan/dhawan-4.jpeg  \n",
      "  inflating: dataset/dhawan/dhawan-5.jpeg  \n",
      "  inflating: dataset/dhawan/dhawan-6.jpg  \n",
      "  inflating: dataset/dhawan/dhawan-7.jpg  \n",
      "  inflating: dataset/dhawan/dhawn-10.jpg  \n",
      "  inflating: dataset/dhawan/dhawn-8.jpg  \n",
      "  inflating: dataset/dhawan/dhawn-9.jpg  \n",
      "   creating: dataset/dhoni/\n",
      "  inflating: dataset/dhoni/dhoni-1.jpeg  \n",
      "  inflating: dataset/dhoni/dhoni-10.jpg  \n",
      "  inflating: dataset/dhoni/dhoni-2.jpeg  \n",
      "  inflating: dataset/dhoni/dhoni-3.jpeg  \n",
      "  inflating: dataset/dhoni/dhoni-4.jpeg  \n",
      "  inflating: dataset/dhoni/dhoni-5.jpeg  \n",
      "  inflating: dataset/dhoni/dhoni-6.jpg  \n",
      "  inflating: dataset/dhoni/dhoni-7.jpeg  \n",
      "  inflating: dataset/dhoni/dhoni-8.jpg  \n",
      "  inflating: dataset/dhoni/dhoni-9.jpg  \n",
      "   creating: dataset/DYPatil/\n",
      "  inflating: dataset/DYPatil/DYPatil.jpeg  \n",
      "  inflating: dataset/DYPatil/DYPatil-1.jpeg  \n",
      "  inflating: dataset/DYPatil/DYPatil-10.jpg  \n",
      "  inflating: dataset/DYPatil/DYPatil-2.jpeg  \n",
      "  inflating: dataset/DYPatil/DYPatil-3.jpeg  \n",
      "  inflating: dataset/DYPatil/DYPatil-4.jpeg  \n",
      "  inflating: dataset/DYPatil/DYPatil-5.jpeg  \n",
      "  inflating: dataset/DYPatil/DYPatil-7.jpg  \n",
      "  inflating: dataset/DYPatil/DYPatil-8.jpg  \n",
      "  inflating: dataset/DYPatil/DYPatil-9.jpg  \n",
      "   creating: dataset/jadeja/\n",
      "  inflating: dataset/jadeja/jadeja-1.jpg  \n",
      "  inflating: dataset/jadeja/jadeja-10.jpg  \n",
      "  inflating: dataset/jadeja/jadeja-2.jpg  \n",
      "  inflating: dataset/jadeja/jadeja-3.jpg  \n",
      "  inflating: dataset/jadeja/jadeja-4.jpg  \n",
      "  inflating: dataset/jadeja/jadeja-5.jpg  \n",
      "  inflating: dataset/jadeja/jadeja-6.jpg  \n",
      "  inflating: dataset/jadeja/jadeja-7.jpg  \n",
      "  inflating: dataset/jadeja/jadeja-8.jpeg  \n",
      "  inflating: dataset/jadeja/jadeja-9.jpeg  \n",
      "   creating: dataset/modi/\n",
      "  inflating: dataset/modi/modi-1.jpeg  \n",
      "  inflating: dataset/modi/modi-10.jpg  \n",
      "  inflating: dataset/modi/modi-2.jpeg  \n",
      "  inflating: dataset/modi/modi-3.jpeg  \n",
      "  inflating: dataset/modi/modi-4.jpeg  \n",
      "  inflating: dataset/modi/modi-5.jpeg  \n",
      "  inflating: dataset/modi/modi-6.jpg  \n",
      "  inflating: dataset/modi/modi-7.jpeg  \n",
      "  inflating: dataset/modi/modi-8.jpg  \n",
      "  inflating: dataset/modi/modi-9.jpg  \n",
      "   creating: dataset/Obulapathi/\n",
      "  inflating: dataset/Obulapathi/obulapathi-1.jpeg  \n",
      "  inflating: dataset/Obulapathi/obulapathi-2.jpeg  \n",
      "   creating: dataset/prabhat_ranjan/\n",
      "  inflating: dataset/prabhat_ranjan/prabhat_ranjan-1.jpeg  \n",
      "  inflating: dataset/prabhat_ranjan/prabhat_ranjan-2.jpeg  \n",
      "  inflating: dataset/prabhat_ranjan/prabhat_ranjan-3.jpeg  \n",
      "  inflating: dataset/prabhat_ranjan/prabhat_ranjan-4.jpeg  \n",
      "  inflating: dataset/prabhat_ranjan/prabhat_ranjan-5.jpeg  \n",
      "  inflating: dataset/prabhat_ranjan/Prabhat-10.jpg  \n",
      "  inflating: dataset/prabhat_ranjan/prabhat-6.jpg  \n",
      "  inflating: dataset/prabhat_ranjan/Prabhat-Ranjan-8.jpg  \n",
      "  inflating: dataset/prabhat_ranjan/PrabhatRanjan-9.jpg  \n",
      "   creating: dataset/prasad/\n",
      "  inflating: dataset/prasad/prasad-1.jpg  \n",
      "  inflating: dataset/prasad/prasad-10.jpg  \n",
      "  inflating: dataset/prasad/prasad-2.jpg  \n",
      " extracting: dataset/prasad/prasad-3.jpeg  \n",
      "  inflating: dataset/prasad/prasad-4.jpg  \n",
      "  inflating: dataset/prasad/prasad-5.jpg  \n",
      "  inflating: dataset/prasad/prasad-6.jpg  \n",
      "  inflating: dataset/prasad/prasad-7.jpg  \n",
      "  inflating: dataset/prasad/prasad-8.jpg  \n",
      "  inflating: dataset/prasad/prasad-9.jpg  \n",
      "   creating: dataset/Rahane/\n",
      "  inflating: dataset/Rahane/rahane.jpg  \n",
      "  inflating: dataset/Rahane/rahane-1.jpeg  \n",
      "  inflating: dataset/Rahane/rahane-10.jpg  \n",
      "  inflating: dataset/Rahane/rahane-2.jpg  \n",
      "  inflating: dataset/Rahane/rahane-5.jpg  \n",
      "  inflating: dataset/Rahane/rahane-6.jpg  \n",
      "  inflating: dataset/Rahane/rahane-7.jpg  \n",
      "  inflating: dataset/Rahane/rahane-8.jpg  \n",
      "   creating: dataset/raina/\n",
      "  inflating: dataset/raina/raina-1.jpg  \n",
      "  inflating: dataset/raina/raina-10.jpg  \n",
      "  inflating: dataset/raina/raina-2.jpg  \n",
      "  inflating: dataset/raina/raina-4.jpg  \n",
      "  inflating: dataset/raina/raina-5.jpg  \n",
      "  inflating: dataset/raina/raina-6.jpg  \n",
      "  inflating: dataset/raina/raina-8.jpg  \n",
      "  inflating: dataset/raina/raina-9.jpeg  \n",
      "   creating: dataset/rohit/\n",
      "  inflating: dataset/rohit/rahit-1.jpeg  \n",
      "  inflating: dataset/rohit/rohit-10.jpg  \n",
      "  inflating: dataset/rohit/rohit-2.jpeg  \n",
      "  inflating: dataset/rohit/rohit-3.jpeg  \n",
      "  inflating: dataset/rohit/rohit-4.jpeg  \n",
      "  inflating: dataset/rohit/rohit-5.jpeg  \n",
      "  inflating: dataset/rohit/rohit-6.jpg  \n",
      "  inflating: dataset/rohit/rohit-7.jpg  \n",
      "  inflating: dataset/rohit/rohit-8.jpg  \n",
      "  inflating: dataset/rohit/rohit-9.jpg  \n",
      "   creating: dataset/sachin/\n",
      "  inflating: dataset/sachin/sachin.jpeg  \n",
      "  inflating: dataset/sachin/sachin-10.jpg  \n",
      "  inflating: dataset/sachin/sachin-11.jpg  \n",
      "  inflating: dataset/sachin/sachin-12.jpg  \n",
      "  inflating: dataset/sachin/sachin-2.jpeg  \n",
      "  inflating: dataset/sachin/sachin-3.jpeg  \n",
      "  inflating: dataset/sachin/sachin-4.jpeg  \n",
      "  inflating: dataset/sachin/sachin-5.jpeg  \n",
      "  inflating: dataset/sachin/sachin-6.jpeg  \n",
      "  inflating: dataset/sachin/sachin-7.jpg  \n",
      "  inflating: dataset/sachin/sachin-8.jpg  \n",
      "  inflating: dataset/sachin/sachin-9.jpg  \n",
      "   creating: dataset/shewag/\n",
      "  inflating: dataset/shewag/sehwag-1.jpg  \n",
      "  inflating: dataset/shewag/sehwag-10.jpg  \n",
      "  inflating: dataset/shewag/sehwag-2.jpg  \n",
      "  inflating: dataset/shewag/sehwag-5.jpg  \n",
      "  inflating: dataset/shewag/sehwag-7.jpg  \n",
      "  inflating: dataset/shewag/shewag-3.jpeg  \n",
      "  inflating: dataset/shewag/shewag-4.jpg  \n",
      "  inflating: dataset/shewag/shewag-6.jpg  \n",
      "  inflating: dataset/shewag/shewag-8.jpg  \n",
      "  inflating: dataset/shewag/shewag-9.jpg  \n",
      "   creating: dataset/Umesh/\n",
      "  inflating: dataset/Umesh/umash-10.jpg  \n",
      "  inflating: dataset/Umesh/umash-2.jpg  \n",
      "  inflating: dataset/Umesh/umash-3.jpg  \n",
      "  inflating: dataset/Umesh/umash-4.jpg  \n",
      "  inflating: dataset/Umesh/umash-5.jpg  \n",
      "  inflating: dataset/Umesh/umash-6.jpg  \n",
      "  inflating: dataset/Umesh/umash-9.jpg  \n",
      "   creating: dataset/viratKohili/\n",
      "  inflating: dataset/viratKohili/vk-1.jpeg  \n",
      "  inflating: dataset/viratKohili/vk-10.jpg  \n",
      "  inflating: dataset/viratKohili/vk-11.jpg  \n",
      "  inflating: dataset/viratKohili/vk-2.jpeg  \n",
      "  inflating: dataset/viratKohili/vk-3.jpeg  \n",
      "  inflating: dataset/viratKohili/vk-4.jpeg  \n",
      "  inflating: dataset/viratKohili/vk-5.jpeg  \n",
      "  inflating: dataset/viratKohili/vk-6.jpeg  \n",
      "  inflating: dataset/viratKohili/vk-7.jpg  \n",
      "  inflating: dataset/viratKohili/vk-8.jpeg  \n",
      "  inflating: dataset/viratKohili/vk-9.jpg  \n",
      "   creating: dataset/Yuvraj/\n",
      "  inflating: dataset/Yuvraj/uv-1.jpg  \n",
      "  inflating: dataset/Yuvraj/uv-10.jpg  \n",
      "  inflating: dataset/Yuvraj/uv-2.jpg  \n",
      "  inflating: dataset/Yuvraj/uv-3.jpg  \n",
      "  inflating: dataset/Yuvraj/uv-4.jpeg  \n",
      "  inflating: dataset/Yuvraj/uv-5.jpg  \n",
      "  inflating: dataset/Yuvraj/uv-6.jpg  \n",
      "  inflating: dataset/Yuvraj/uv-7.jpg  \n",
      "  inflating: dataset/Yuvraj/uv-8.jpg  \n",
      "  inflating: dataset/Yuvraj/uv-9.jpg  \n"
     ]
    }
   ],
   "source": [
    "!unzip dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Oun6DPoY7MLH",
    "outputId": "df96ff02-8c77-40c9-ced5-e9c302911e47"
   },
   "outputs": [],
   "source": [
    "# face detection model paths\n",
    "protoPath = os.getcwd()+\"/deploy.prototxt.txt\"\n",
    "modelPath = os.getcwd()+\"/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "\n",
    "# loading detection model\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "\n",
    "# load embedding model\n",
    "embedder = embedding.InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "# paths to save pickle files\n",
    "currentDir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Lw2OaiP8Bp7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file output already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Touching output/SimpleEmbeddings.pickle\n"
     ]
    }
   ],
   "source": [
    "# images folder\n",
    "dataset = os.path.join(currentDir, \"dataset\")\n",
    "\n",
    "# paths to save pickle files\n",
    "!mkdir output\n",
    "!touch  output/SimpleEmbeddings.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZCfFbzb8JC2"
   },
   "outputs": [],
   "source": [
    "# getting all images paths\n",
    "\n",
    "imagePaths = []\n",
    "\n",
    "for person in os.listdir(dataset):\n",
    "    for img in os.listdir(dataset+\"/\"+person):\n",
    "        imagePaths.append(dataset+\"/\"+person+\"/\"+img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XtlsxgLk9KAh"
   },
   "outputs": [],
   "source": [
    "# create lists to append ImgPaths/names/imageIDs/boxs/embeddings\n",
    "ImgPaths = []\n",
    "names = []\n",
    "imageIDs = []\n",
    "boxs = []\n",
    "embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'APJ Abdul Kalam'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagePaths[0].split(\"/\")[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    name = imagePath.split(\"/\")[-2]\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuPyXT_Z9OXC"
   },
   "outputs": [],
   "source": [
    "# initlize the total number of faces processed\n",
    "total = 0\n",
    "\n",
    "# loop over the image paths\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "  \n",
    "    #print(i,imagePath)\n",
    "    \n",
    "    #extract the person name from the image path\n",
    "    \n",
    "    name = imagePath.split(\"/\")[-2]\n",
    "    imageID = imagePath.split(os.path.sep)[-1].split('.')[-2]\n",
    "    \n",
    "    image = cv2.imread(imagePath)\n",
    "    (h,w) = image.shape[:2]\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "    \n",
    "    detector.setInput(blob)\n",
    "    detections = detector.forward()\n",
    "    \n",
    "    if len(detections) > 0:\n",
    "        \n",
    "        # we're making the assumption that each image has only ONE\n",
    "        # face, so find the bounding box with the largest probalility\n",
    "        \n",
    "        i = np.argmax(detections[0, 0, :, 2])\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        \n",
    "        # ensure that the detection with the largest probability also\n",
    "        # means our minimum probability test (thus helping filter out\n",
    "        # weak detections)\n",
    "        \n",
    "        if confidence > 0.5:\n",
    "            \n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            \n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            \n",
    "            face = image[startY:endY , startX:endX]\n",
    "            (fH , fW) = face.shape[:2]\n",
    "            \n",
    "            \n",
    "            # ensure the facce width and height are sufficently large\n",
    "            if fW < 20 or fH < 20:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,(160, 160), (0, 0, 0), swapRB=True, crop=False)\n",
    "            except:\n",
    "                print(\"[Error] - Face size in Image not sufficent to get Embeddings : \", imagePath)\n",
    "                continue\n",
    "            \n",
    "            faceTensor = torch.tensor(faceBlob)\n",
    "            faceEmbed = embedder(faceTensor)\n",
    "            flattenEmbed = faceEmbed.squeeze(0).detach().numpy()\n",
    "            \n",
    "            ImgPaths.append(imagePath)\n",
    "            imageIDs.append(imageID)\n",
    "            names.append(name)\n",
    "            boxs.append(box)\n",
    "            embeddings.append(flattenEmbed)\n",
    "            total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['APJ Abdul Kalam',\n",
       " 'APJ Abdul Kalam',\n",
       " 'APJ Abdul Kalam',\n",
       " 'APJ Abdul Kalam',\n",
       " 'APJ Abdul Kalam',\n",
       " 'APJ Abdul Kalam',\n",
       " 'APJ Abdul Kalam',\n",
       " 'APJ Abdul Kalam',\n",
       " 'APJ Abdul Kalam',\n",
       " 'APJ Abdul Kalam',\n",
       " 'Ashwin',\n",
       " 'Ashwin',\n",
       " 'Ashwin',\n",
       " 'Ashwin',\n",
       " 'Ashwin',\n",
       " 'Ashwin',\n",
       " 'Ashwin',\n",
       " 'Ashwin',\n",
       " 'Ashwin',\n",
       " 'B kumar',\n",
       " 'B kumar',\n",
       " 'B kumar',\n",
       " 'B kumar',\n",
       " 'B kumar',\n",
       " 'B kumar',\n",
       " 'B kumar',\n",
       " 'B kumar',\n",
       " 'bumrah',\n",
       " 'bumrah',\n",
       " 'bumrah',\n",
       " 'bumrah',\n",
       " 'bumrah',\n",
       " 'bumrah',\n",
       " 'bumrah',\n",
       " 'bumrah',\n",
       " 'bumrah',\n",
       " 'dhawan',\n",
       " 'dhawan',\n",
       " 'dhawan',\n",
       " 'dhawan',\n",
       " 'dhawan',\n",
       " 'dhawan',\n",
       " 'dhawan',\n",
       " 'dhawan',\n",
       " 'dhawan',\n",
       " 'dhawan',\n",
       " 'dhoni',\n",
       " 'dhoni',\n",
       " 'dhoni',\n",
       " 'dhoni',\n",
       " 'dhoni',\n",
       " 'dhoni',\n",
       " 'dhoni',\n",
       " 'dhoni',\n",
       " 'dhoni',\n",
       " 'dhoni',\n",
       " 'DYPatil',\n",
       " 'DYPatil',\n",
       " 'DYPatil',\n",
       " 'DYPatil',\n",
       " 'DYPatil',\n",
       " 'DYPatil',\n",
       " 'DYPatil',\n",
       " 'DYPatil',\n",
       " 'DYPatil',\n",
       " 'DYPatil',\n",
       " 'jadeja',\n",
       " 'jadeja',\n",
       " 'jadeja',\n",
       " 'jadeja',\n",
       " 'jadeja',\n",
       " 'jadeja',\n",
       " 'jadeja',\n",
       " 'jadeja',\n",
       " 'jadeja',\n",
       " 'jadeja',\n",
       " 'modi',\n",
       " 'modi',\n",
       " 'modi',\n",
       " 'modi',\n",
       " 'modi',\n",
       " 'modi',\n",
       " 'modi',\n",
       " 'modi',\n",
       " 'modi',\n",
       " 'modi',\n",
       " 'Obulapathi',\n",
       " 'Obulapathi',\n",
       " 'prabhat_ranjan',\n",
       " 'prabhat_ranjan',\n",
       " 'prabhat_ranjan',\n",
       " 'prabhat_ranjan',\n",
       " 'prabhat_ranjan',\n",
       " 'prabhat_ranjan',\n",
       " 'prabhat_ranjan',\n",
       " 'prabhat_ranjan',\n",
       " 'prabhat_ranjan',\n",
       " 'prasad',\n",
       " 'prasad',\n",
       " 'prasad',\n",
       " 'prasad',\n",
       " 'prasad',\n",
       " 'prasad',\n",
       " 'prasad',\n",
       " 'prasad',\n",
       " 'prasad',\n",
       " 'prasad',\n",
       " 'Rahane',\n",
       " 'Rahane',\n",
       " 'Rahane',\n",
       " 'Rahane',\n",
       " 'Rahane',\n",
       " 'Rahane',\n",
       " 'Rahane',\n",
       " 'Rahane',\n",
       " 'raina',\n",
       " 'raina',\n",
       " 'raina',\n",
       " 'raina',\n",
       " 'raina',\n",
       " 'raina',\n",
       " 'raina',\n",
       " 'raina',\n",
       " 'rohit',\n",
       " 'rohit',\n",
       " 'rohit',\n",
       " 'rohit',\n",
       " 'rohit',\n",
       " 'rohit',\n",
       " 'rohit',\n",
       " 'rohit',\n",
       " 'rohit',\n",
       " 'rohit',\n",
       " 'sachin',\n",
       " 'sachin',\n",
       " 'sachin',\n",
       " 'sachin',\n",
       " 'sachin',\n",
       " 'sachin',\n",
       " 'sachin',\n",
       " 'sachin',\n",
       " 'sachin',\n",
       " 'sachin',\n",
       " 'sachin',\n",
       " 'sachin',\n",
       " 'shewag',\n",
       " 'shewag',\n",
       " 'shewag',\n",
       " 'shewag',\n",
       " 'shewag',\n",
       " 'shewag',\n",
       " 'shewag',\n",
       " 'shewag',\n",
       " 'shewag',\n",
       " 'shewag',\n",
       " 'Umesh',\n",
       " 'Umesh',\n",
       " 'Umesh',\n",
       " 'Umesh',\n",
       " 'Umesh',\n",
       " 'Umesh',\n",
       " 'Umesh',\n",
       " 'viratKohili',\n",
       " 'viratKohili',\n",
       " 'viratKohili',\n",
       " 'viratKohili',\n",
       " 'viratKohili',\n",
       " 'viratKohili',\n",
       " 'viratKohili',\n",
       " 'viratKohili',\n",
       " 'viratKohili',\n",
       " 'viratKohili',\n",
       " 'viratKohili',\n",
       " 'Yuvraj',\n",
       " 'Yuvraj',\n",
       " 'Yuvraj',\n",
       " 'Yuvraj',\n",
       " 'Yuvraj',\n",
       " 'Yuvraj',\n",
       " 'Yuvraj',\n",
       " 'Yuvraj',\n",
       " 'Yuvraj',\n",
       " 'Yuvraj']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ttXo2Ioy-KBn",
    "outputId": "ac23447e-17a5-46c0-a947-a87340789a7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] serializing 183 encodings ....\n"
     ]
    }
   ],
   "source": [
    "# dump the facial embeddings + names to disk\n",
    "print(\"[INFO] serializing {} encodings ....\".format(total))\n",
    "data = {\"paths\":ImgPaths, \"names\":names, \"imageIDs\":imageIDs, \"boxs\":boxs, \"embeddings\":embeddings}\n",
    "file_path = \"F:/FACENET/output/SimpleEmbeddings.pickle\"\n",
    "with open(file_path , \"wb\") as f:\n",
    "    pickle.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HezAdkzVBbG4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Touching output/SimpleRecognizer.pickle\n",
      "Touching output/SimpleLabel.pickle\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# paths to embedding pickle file\n",
    "embeddingPickle = \"./output/SimpleEmbeddings.pickle\"\n",
    "\n",
    "# path to recognizer pickle file\n",
    "!touch output/SimpleRecognizer.pickle\n",
    "recognizerPickle = \"./output/SimpleRecognizer.pickle\"\n",
    "\n",
    "# path to labels pickle file\n",
    "!touch output/SimpleLabel.pickle\n",
    "labelPickle = \"./output/SimpleLabel.pickle\"\n",
    "\n",
    "# loading embeddings pickle\n",
    "data = pickle.loads(open(embeddingPickle, \"rb\").read())\n",
    "\n",
    "# encode the labels\n",
    "label = LabelEncoder()\n",
    "labels = label.fit_transform(data[\"names\"])\n",
    "\n",
    "# getting embeddings\n",
    "Embeddings = np.array(data[\"embeddings\"])\n",
    "\n",
    "# train the model used to accept the 512-d embeddings of the face and \n",
    "# then produce the actual face recognition\n",
    "\n",
    "recognizer = KNeighborsClassifier(n_neighbors= 2, metric='euclidean', weights=\"distance\")\n",
    "#recognizer = SVC(probability=True)\n",
    "recognizer.fit(Embeddings, labels)\n",
    "\n",
    "# write the actual face recognition model to disk\n",
    "f = open(recognizerPickle, \"wb\")\n",
    "f.write(pickle.dumps(recognizer))\n",
    "f.close()\n",
    "\n",
    "# write the label encoder to disk\n",
    "f = open(labelPickle,\"wb\")\n",
    "f.write(pickle.dumps(label))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vH19Lo__E--a"
   },
   "outputs": [],
   "source": [
    "# loading face detection model\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "\n",
    "# load embedding model\n",
    "embedder = embedding.InceptionResnetV1(pretrained=\"vggface2\").eval()\n",
    "\n",
    "# load the actual face recognition model along with the label encoder\n",
    "recognizer = pickle.loads(open(recognizerPickle, \"rb\").read())\n",
    "label = pickle.loads(open(labelPickle, \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KBS5V1eBFiyr",
    "outputId": "386eb152-dfdf-4a1b-dc1a-b2247fc414f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagePath = os.getcwd() + \"/test3.jpg\"\n",
    "\n",
    "predictedImg = os.getcwd()\n",
    "\n",
    "image = cv2.imread(imagePath)\n",
    "(h,w) = image.shape[:2]\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "detector.setInput(blob)\n",
    "detections = detector.forward()\n",
    "\n",
    "# loop over the detections\n",
    "for i in range(0, detections.shape[2]):\n",
    "    \n",
    "    # extract the confidence (i.e., probalility) associated with the prediction\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "    \n",
    "    # fillter out weak detections\n",
    "    if confidence > 0.2:\n",
    "        \n",
    "        # compute the (x ,y) - coordinates of the bounding box for the face\n",
    "        \n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "        \n",
    "        # extract the face ROI\n",
    "        face = image[startY:endY , startX:endX]\n",
    "        (fH ,fW) = face.shape[:2]\n",
    "        \n",
    "        # ensure the facce width and height are sufficently large\n",
    "        if fW < 20 or fH < 20:\n",
    "            print(\"[Error] - Face size in Image not sufficent to get Embeddings : \", imagePath)\n",
    "            continue\n",
    "        \n",
    "\n",
    "        try:\n",
    "            faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,(160, 160), (0, 0, 0), swapRB=True, crop=False)\n",
    "        except:\n",
    "            print(\"[Error] - Face size in Image not sufficent to get Embeddings : \", imagePath)\n",
    "            continue\n",
    "        \n",
    "        faceTensor = torch.tensor(faceBlob)\n",
    "        faceEmbed = embedder(faceTensor)\n",
    "        flattenEmbed = faceEmbed.squeeze(0).detach().numpy()\n",
    "        \n",
    "        array = np.array(flattenEmbed).reshape(1,-1)\n",
    "        \n",
    "        # perform classification to recognize the face\n",
    "        \n",
    "        preds = recognizer.predict_proba(array)[0]\n",
    "        \n",
    "        j = np.argmax(preds)\n",
    "        \n",
    "        proba = preds[j]\n",
    "        name = label.classes_[j]\n",
    "        \n",
    "        #draw the bunding box of the face along with the associated probability\n",
    "        \n",
    "        text = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        \n",
    "        cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
    "        \n",
    "        cv2.putText(image, text, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.50, (255, 255, 255), 1)\n",
    "        \n",
    "# save image predicte folder\n",
    "cv2.imwrite(\"{}/test_prediction.png\".format(predictedImg), image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FaceNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
